{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition -Model8\n",
    "\n",
    "使用MTCNN進行人臉偵測\n",
    "\n",
    "使用自建CNN模型進行人臉辨識\n",
    "\n",
    "使用kaggle5位日本明星資料集\n",
    "\n",
    "進行靜態圖像辨識\n",
    "\n",
    "Transfer learning-VGG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "# 將警告訊息關掉\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Utilities相關函式庫\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Python2.X和3.X相容\n",
    "import six\n",
    "\n",
    "# 圖像處理函示庫\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "# 數值處理函式庫\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 機器學習函式庫\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 深度學習函式庫\n",
    "import keras \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, Convolution2D, Dropout, Flatten, merge, Reshape, Activation, Lambda, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, add, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.regularizers import l2\n",
    "from keras import initializers\n",
    "from keras.engine import Layer, InputSpec, get_source_inputs\n",
    "from keras_vggface.vggface import VGGFace\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "\n",
    "\n",
    "# Backend\n",
    "from keras import backend as K\n",
    "\n",
    "# 人臉偵測函式庫\n",
    "from mtcnn import MTCNN\n",
    "detector = MTCNN()\n",
    "\n",
    "# 相關參數設定\n",
    "picture_size = 224\n",
    "batch_size = 8  # 一次用多少筆資料更新模型\n",
    "num_classes = 5  # 資料集有幾個類別\n",
    "epochs = 200      # 訓練迭代次數\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models') # 儲存模型的路徑\n",
    "model_name = 'Model8_Transfer_VGG_V2, trained_model.h5' # 模型名稱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 224, 224, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接讀取處理好的檔案\n",
    "total_img_resized = np.load('total_crop_resized.npy')\n",
    "labelnames = np.load('labelnames.npy')\n",
    "total_img_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (403, 224, 224, 3) test: (45, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# 切分訓練/測試資料\n",
    "#train_x = total_img\n",
    "#train_y = labelnames\n",
    "\n",
    "train_x,test_x, train_y, test_y = train_test_split(total_img_resized,\n",
    "                                                  labelnames,\n",
    "                                                test_size = 0.1,stratify= labelnames)#random_state = 123,\n",
    "print('train:',train_x.shape,'test:',test_x.shape)\n",
    "\n",
    "# 正規化資料\n",
    "# 標準化0~255的值到0~1\n",
    "# x_train_normalize = train_x.astype('float32') / 255.0\n",
    "# x_test_normalize = test_x.astype('float32') / 255.0\n",
    "x_train_normalize = preprocess_input(train_x)\n",
    "x_test_normalize = preprocess_input(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 112, 112, 64) 9408        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2/bn (BatchNormaliza (None, 112, 112, 64) 256         conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 112, 112, 64) 0           conv1/7x7_s2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce (Conv2D)     (None, 55, 55, 64)   4096        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 55, 55, 64)   0           conv2_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 55, 55, 64)   0           conv2_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj (Conv2D)       (None, 55, 55, 256)  16384       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj/bn (BatchNorma (None, 55, 55, 256)  1024        conv2_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 55, 55, 256)  0           conv2_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv2_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 55, 55, 256)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 55, 55, 64)   0           conv2_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 55, 55, 64)   0           conv2_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 55, 55, 256)  0           conv2_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 55, 55, 256)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 55, 55, 64)   0           conv2_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 55, 55, 64)   0           conv2_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 55, 55, 256)  0           conv2_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 55, 55, 256)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce (Conv2D)     (None, 28, 28, 128)  32768       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 28, 28, 128)  0           conv3_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 28, 28, 128)  0           conv3_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj (Conv2D)       (None, 28, 28, 512)  131072      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj/bn (BatchNorma (None, 28, 28, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 28, 28, 512)  0           conv3_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv3_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 28, 28, 512)  0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 28, 28, 128)  0           conv3_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 28, 28, 128)  0           conv3_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 28, 28, 512)  0           conv3_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 28, 28, 512)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 28, 28, 128)  0           conv3_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 28, 28, 128)  0           conv3_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 28, 28, 512)  0           conv3_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 28, 28, 512)  0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 28, 28, 128)  0           conv3_4_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 28, 28, 128)  0           conv3_4_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 28, 28, 512)  0           conv3_4_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 28, 28, 512)  0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce (Conv2D)     (None, 14, 14, 256)  131072      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 256)  0           conv4_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 14, 14, 256)  0           conv4_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj (Conv2D)       (None, 14, 14, 1024) 524288      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj/bn (BatchNorma (None, 14, 14, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 14, 14, 1024) 0           conv4_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv4_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 14, 14, 1024) 0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 14, 14, 256)  0           conv4_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 14, 14, 256)  0           conv4_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 14, 14, 1024) 0           conv4_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 1024) 0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 14, 14, 256)  0           conv4_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 14, 14, 256)  0           conv4_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 14, 14, 1024) 0           conv4_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 14, 14, 1024) 0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 14, 14, 256)  0           conv4_4_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 14, 14, 256)  0           conv4_4_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 14, 14, 1024) 0           conv4_4_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 14, 14, 1024) 0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 14, 14, 256)  0           conv4_5_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_5_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 14, 14, 256)  0           conv4_5_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 14, 14, 1024) 0           conv4_5_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 14, 14, 1024) 0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 14, 14, 256)  0           conv4_6_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_6_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 14, 14, 256)  0           conv4_6_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 14, 14, 1024) 0           conv4_6_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 14, 14, 1024) 0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce (Conv2D)     (None, 7, 7, 512)    524288      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 7, 7, 512)    0           conv5_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 7, 7, 512)    0           conv5_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj (Conv2D)       (None, 7, 7, 2048)   2097152     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj/bn (BatchNorma (None, 7, 7, 2048)   8192        conv5_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 7, 7, 2048)   0           conv5_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv5_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 7, 7, 2048)   0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 7, 7, 512)    0           conv5_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 7, 7, 512)    0           conv5_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 7, 7, 2048)   0           conv5_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 7, 7, 2048)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 7, 7, 512)    0           conv5_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 7, 7, 512)    0           conv5_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 7, 7, 2048)   0           conv5_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 7, 7, 2048)   0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4096)         8392704     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "classifier (Dense)              (None, 5)            20485       dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 31,974,341\n",
      "Trainable params: 8,413,189\n",
      "Non-trainable params: 23,561,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Transfer learning-VGG\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "img_rows, img_cols, img_channel = picture_size, picture_size, 3\n",
    "\n",
    "\n",
    "#custom parameters\n",
    "nb_class = 5\n",
    "\n",
    "base_model = VGGFace(model='resnet50',include_top=False, input_shape=(img_rows, img_cols, img_channel))\n",
    "\n",
    "\n",
    "# 凍結基底模型的權重，不更新\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "last_layer = base_model.get_layer('avg_pool').output\n",
    "x = Flatten(name='flatten')(last_layer)\n",
    "x = Dense(4096)(x)\n",
    "out = Dense(nb_class, activation='softmax', name='classifier')(x)\n",
    "model = Model(base_model.input, out)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型\n",
    "optimizer = Adam(lr=3e-6,decay=0.0001,beta_1=0.99,beta_2=0.999) #100e-6\n",
    "# sgd = keras.optimizers.SGD(lr=3e-6, momentum=0.9, decay=1e-5, nesterov=True)\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = optimizer,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "datagen  = ImageDataGenerator(\n",
    "            rotation_range = 20,\n",
    "            width_shift_range = 0.2,\n",
    "            height_shift_range = 0.2,\n",
    "            horizontal_flip = True, \n",
    "            vertical_flip = False,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "100/100 [==============================] - 34s 338ms/step - loss: 1.4018 - acc: 0.4933 - val_loss: 0.6146 - val_acc: 0.7556\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61455, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 0.5456 - acc: 0.8129 - val_loss: 0.2504 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61455 to 0.25043, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.3802 - acc: 0.8742 - val_loss: 0.1521 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25043 to 0.15208, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.3002 - acc: 0.8879 - val_loss: 0.1532 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15208\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 0.2927 - acc: 0.9012 - val_loss: 0.1065 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15208 to 0.10655, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.2332 - acc: 0.9300 - val_loss: 0.1343 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10655\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.2317 - acc: 0.9279 - val_loss: 0.0915 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10655 to 0.09152, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.2082 - acc: 0.9262 - val_loss: 0.1060 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09152\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1696 - acc: 0.9454 - val_loss: 0.1197 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09152\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.2132 - acc: 0.9175 - val_loss: 0.0848 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09152 to 0.08482, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.2147 - acc: 0.9242 - val_loss: 0.0964 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.08482\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1877 - acc: 0.9412 - val_loss: 0.0827 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08482 to 0.08266, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.1515 - acc: 0.9500 - val_loss: 0.0708 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08266 to 0.07082, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.1344 - acc: 0.9625 - val_loss: 0.0784 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07082\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1555 - acc: 0.9562 - val_loss: 0.0771 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.07082\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.1704 - acc: 0.9359 - val_loss: 0.0684 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07082 to 0.06835, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 23s 235ms/step - loss: 0.1615 - acc: 0.9496 - val_loss: 0.0677 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.06835 to 0.06773, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1427 - acc: 0.9512 - val_loss: 0.0795 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06773\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1481 - acc: 0.9500 - val_loss: 0.0770 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.06773\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.1546 - acc: 0.9459 - val_loss: 0.0614 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.06773 to 0.06143, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1358 - acc: 0.9567 - val_loss: 0.0779 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.06143\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1391 - acc: 0.9562 - val_loss: 0.0854 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06143\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1284 - acc: 0.9562 - val_loss: 0.0786 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.06143\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1352 - acc: 0.9604 - val_loss: 0.0684 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.06143\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.1322 - acc: 0.9471 - val_loss: 0.0643 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06143\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.1382 - acc: 0.9512 - val_loss: 0.0575 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06143 to 0.05751, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1117 - acc: 0.9567 - val_loss: 0.0497 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05751 to 0.04970, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.1291 - acc: 0.9579 - val_loss: 0.0494 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.04970 to 0.04936, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1171 - acc: 0.9596 - val_loss: 0.0602 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04936\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1149 - acc: 0.9587 - val_loss: 0.0562 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.04936\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1074 - acc: 0.9712 - val_loss: 0.0412 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04936 to 0.04122, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1384 - acc: 0.9617 - val_loss: 0.0591 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.04122\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1237 - acc: 0.9587 - val_loss: 0.0456 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.04122\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0888 - acc: 0.9687 - val_loss: 0.0357 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04122 to 0.03573, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 35/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1202 - acc: 0.9575 - val_loss: 0.0488 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.03573\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1121 - acc: 0.9604 - val_loss: 0.0332 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.03573 to 0.03321, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0968 - acc: 0.9650 - val_loss: 0.0330 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.03321 to 0.03304, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.1012 - acc: 0.9687 - val_loss: 0.0556 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03304\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0943 - acc: 0.9712 - val_loss: 0.0428 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03304\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0860 - acc: 0.9762 - val_loss: 0.0325 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03304 to 0.03247, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.0949 - acc: 0.9612 - val_loss: 0.0341 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03247\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.1116 - acc: 0.9554 - val_loss: 0.0349 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03247\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.1013 - acc: 0.9662 - val_loss: 0.0262 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.03247 to 0.02621, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0781 - acc: 0.9712 - val_loss: 0.0355 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02621\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0906 - acc: 0.9675 - val_loss: 0.0383 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02621\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0939 - acc: 0.9650 - val_loss: 0.0288 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02621\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0971 - acc: 0.9675 - val_loss: 0.0367 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02621\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0911 - acc: 0.9700 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02621\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0890 - acc: 0.9642 - val_loss: 0.0343 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02621\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0637 - acc: 0.9787 - val_loss: 0.0660 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02621\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0584 - acc: 0.9812 - val_loss: 0.0452 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02621\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.1044 - acc: 0.9600 - val_loss: 0.0336 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02621\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0722 - acc: 0.9787 - val_loss: 0.0392 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02621\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0790 - acc: 0.9779 - val_loss: 0.0491 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02621\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1066 - acc: 0.9625 - val_loss: 0.0427 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02621\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0913 - acc: 0.9712 - val_loss: 0.0484 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02621\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0989 - acc: 0.9692 - val_loss: 0.0483 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02621\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0757 - acc: 0.9750 - val_loss: 0.0437 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02621\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0861 - acc: 0.9750 - val_loss: 0.0354 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02621\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0880 - acc: 0.9784 - val_loss: 0.0435 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02621\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0533 - acc: 0.9850 - val_loss: 0.0360 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02621\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 0.0755 - acc: 0.9754 - val_loss: 0.0258 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.02621 to 0.02585, saving model to d:\\Side Project\\Face Recognition\\saved_models\\Model8_Transfer_VGG_V2, trained_model.h5\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0817 - acc: 0.9712 - val_loss: 0.0333 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02585\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0918 - acc: 0.9729 - val_loss: 0.0497 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02585\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0801 - acc: 0.9754 - val_loss: 0.0344 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02585\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0793 - acc: 0.9712 - val_loss: 0.0273 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02585\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0732 - acc: 0.9737 - val_loss: 0.0339 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02585\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0594 - acc: 0.9837 - val_loss: 0.0359 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02585\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0919 - acc: 0.9779 - val_loss: 0.0284 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02585\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1029 - acc: 0.9700 - val_loss: 0.0305 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02585\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0755 - acc: 0.9750 - val_loss: 0.0387 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02585\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0718 - acc: 0.9792 - val_loss: 0.0284 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02585\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0712 - acc: 0.9792 - val_loss: 0.0353 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02585\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0624 - acc: 0.9784 - val_loss: 0.0451 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02585\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0667 - acc: 0.9725 - val_loss: 0.0303 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02585\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0691 - acc: 0.9787 - val_loss: 0.0263 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02585\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0690 - acc: 0.9721 - val_loss: 0.0344 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02585\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0823 - acc: 0.9729 - val_loss: 0.0337 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02585\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0925 - acc: 0.9642 - val_loss: 0.0375 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02585\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0664 - acc: 0.9804 - val_loss: 0.0396 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02585\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.0991 - acc: 0.9637 - val_loss: 0.0302 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02585\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0719 - acc: 0.9701 - val_loss: 0.0321 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02585\n",
      "Epoch 00082: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Use ModelCheckpoint to save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor = 'val_loss', save_best_only = True, verbose = 1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', patience = 20, verbose = 1) # patience=10為連續10次模型loss沒再下降就停止\n",
    "\n",
    "# Fit model\n",
    "aug_ratio = 2\n",
    "steps_per_epoch = int(aug_ratio * train_x.shape[0] / batch_size)\n",
    "validation_steps = int(aug_ratio * test_x.shape[0] / batch_size)\n",
    "\n",
    "model_history = model.fit_generator(datagen.flow(x_train_normalize, train_y, batch_size = batch_size),\n",
    "                                   epochs = epochs,\n",
    "                                   validation_data = (x_test_normalize, test_y),\n",
    "                                   callbacks = [checkpoint, earlystop],\n",
    "                                   steps_per_epoch=steps_per_epoch,\n",
    "                                   validation_steps=validation_steps\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model\n",
      "45/45 [==============================] - 3s 76ms/step\n",
      "Test loss: 0.025848444551229477\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# loading save model\n",
    "print('Loading trained model')\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Score trained model\n",
    "scores = model.evaluate(x_test_normalize, test_y, verbose = 1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.plot(training_loss, label=\"training_loss\")\n",
    "plt.plot(val_loss, label=\"validation_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
